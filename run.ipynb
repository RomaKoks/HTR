{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.preprocess import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images size\n",
    "img_width = 600\n",
    "img_height = 100\n",
    "\n",
    "# default paths\n",
    "WORKING_DIR = os.path.join('/home', 'mts')\n",
    "ann_path = os.path.join(WORKING_DIR, 'HKR_Dataset_Words_Public', 'ann')\n",
    "img_path = os.path.join(WORKING_DIR, 'HKR_Dataset_Words_Public', 'img')\n",
    "metadata = os.path.join(WORKING_DIR, 'metadata', 'metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64943/64943 [06:52<00:00, 157.56it/s]\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48419, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [19:28<00:00, 12.04s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {image: (None, 600, 100, 1), label: (None, None)}, types: {image: tf.float32, label: tf.int64}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect metadata\n",
    "# meta_collect(ann_path, metadata)\n",
    "\n",
    "# get preprocessed metadata dataframe\n",
    "df = PreprocessFrame(metadata=metadata,\n",
    "                     img_height=img_height, img_width=img_width)\n",
    "print(df.shape)\n",
    "\n",
    "# Make augments file (if they exists: comment or delete line)\n",
    "aug_df = None\n",
    "# aug_df = make_augments(df=df, img_path=img_path, WORKING_DIR=WORKING_DIR,\n",
    "#                         img_height=img_height, img_width=img_width)\n",
    "\n",
    "# get augments metadata dataframe from original dataframe if not starting make_augments\n",
    "if not isinstance(aug_df, pd.DataFrame):\n",
    "    aug_df = df.copy()\n",
    "    aug_df.index = aug_df.index.to_series().apply(lambda x: os.path.join('aug_1', 'aug_' + x))\n",
    "\n",
    "train, test, val = list(Dataset(df, aug_df=aug_df,\n",
    "                                test_size=0.1,\n",
    "                                val_size=0.05,\n",
    "                                img_path=img_path,\n",
    "                                img_height=img_height,\n",
    "                                img_width=img_width,\n",
    "                                WORKING_DIR=WORKING_DIR,\n",
    "                                shuffle=True,\n",
    "                                random_state=12))\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mEDA.html\u001b[0m*                  \u001b[01;32mREADME.md\u001b[0m*   \u001b[34;42mmetadata\u001b[0m/      \u001b[34;42mpreprocess\u001b[0m/\r\n",
      "\u001b[34;42mHKR_Dataset_Words_Public\u001b[0m/  \u001b[34;42marxivs\u001b[0m/      \u001b[34;42mmodel\u001b[0m/         \u001b[01;32mrequirements.txt\u001b[0m*\r\n",
      "\u001b[01;32mLICENSE\u001b[0m*                   \u001b[01;32mdockerfile\u001b[0m*  \u001b[01;32mmts-env.yaml\u001b[0m*  \u001b[01;32mrun.ipynb\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "r'''\n",
    "params:\n",
    "\n",
    "callbacks: list of callback names [checkpoint, csv_log, tb_log, early_stopping]\n",
    "metrics: list of metrics name [cer, wer, accuracy, ctc_loss]\n",
    "\n",
    "checkpoint_path\n",
    "csv_log_path\n",
    "tb_log_path\n",
    "  tb_update_freq: int\n",
    "\n",
    "early_stopping\n",
    "    early_stopping_patience: int\n",
    "\n",
    "input_img_shape: array(width, height, 1)\n",
    "\n",
    "vocab_len: len of vocab with blank\n",
    "restore_weights: bool\n",
    "max_label_len\n",
    "chars_path\n",
    "blank - '#'\n",
    "'''\n",
    "vocab_len = -1 #check from dataset\n",
    "max_label_len = -1\n",
    "chars_path = os.path.join(os.path.split(metadata)[0], 'symbols.txt') # have to save all chars like:\n",
    "r'''\n",
    "a\n",
    "b\n",
    "c\n",
    " \n",
    "#\n",
    "'''\n",
    "\n",
    "params = {\n",
    "    'callbacks': ['checkpoint', 'csv_log', 'tb_log', 'early_stopping'],\n",
    "    'metrics': ['cer', 'wer', 'accuracy'],\n",
    "    'checkpoint_path': 'checkpoints/training_0/cp.ckpt',\n",
    "    'csv_log_path': 'logs/csv_logs/log_0.csv',\n",
    "    'tb_log_path': 'logs/tb_logs/log0',\n",
    "    'tb_update_freq': 200,\n",
    "    'early_stopping_patience': 10,\n",
    "    'input_img_shape': (600, 100, 1),\n",
    "    'vocab_len': -1,\n",
    "    'max_label_len': -1,\n",
    "    'chars_path': 'path.txt',\n",
    "    'blank': '#'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
